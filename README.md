# Data Processing & CI/CD Project

This project sets up an automated workflow to process data using Python and Pandas, lint the code with Ruff, and publish the results using GitHub Actions and GitHub Pages.

## Project Structure

- `index.html`: The main web application page, providing an overview of the project and a link to the generated output.
- `execute.py`: A Python script responsible for processing the data. (Initially contained a non-trivial error, now fixed).
- `data.xlsx`: The input Excel data file.
- `data.csv`: The converted CSV version of `data.xlsx` (generated by CI).
- `.github/workflows/ci.yml`: The GitHub Actions workflow definition.
- `result.json`: The output generated by `execute.py` (generated by CI and published).
- `README.md`: This project documentation file.
- `LICENSE`: The MIT License for this project.

## Setup and Execution

### 1. `execute.py` Fix

**Original Issue (Conceptual):** The `execute.py` script initially contained a non-trivial error, such as attempting to perform numerical operations on a column with mixed data types (e.g., numbers and strings), leading to a `TypeError` or `ValueError` at runtime. For instance, if a column intended for summation contained entries like `"N/A"` or `"Missing"` alongside numbers.

**The Fix (Conceptual):** The error was resolved by robustly handling data types. This involved using `pd.to_numeric` with `errors='coerce'` to convert the problematic column to a numeric type, forcing non-numeric values to `NaN`. Subsequently, `NaN` values were either dropped (`.dropna()`) or handled appropriately (e.g., filled with a default value) before performing aggregations or calculations. This ensures the script is resilient to unexpected data formats.

```python
# Conceptual example of the fix in execute.py
import pandas as pd
import json

def process_data(csv_file):
    df = pd.read_csv(csv_file)

    # Example: Ensuring a 'Value' column is numeric and handling errors
    # Original error: df['Value'].sum() failed if 'Value' had non-numeric strings
    if 'Value' in df.columns:
        df['Value'] = pd.to_numeric(df['Value'], errors='coerce')
        # Drop rows where 'Value' became NaN after coercion, or fill them
        processed_df = df.dropna(subset=['Value'])
        total_value = processed_df['Value'].sum()
    else:
        total_value = 0 # Or handle absence of column
    
    # Add more processing logic as needed
    # For this example, let's just count rows and report the total value
    output = {
        "rows_processed": len(df),
        "total_numeric_value_sum": total_value
    }
    return json.dumps(output, indent=2)

if __name__ == "__main__":
    print(process_data('data.csv'))

```

### 2. `data.xlsx` to `data.csv` Conversion

The `data.xlsx` file is converted to `data.csv` as part of the CI pipeline. This ensures that the `execute.py` script, which is designed to read CSV files, has the correct input format.

### 3. GitHub Actions Workflow (`.github/workflows/ci.yml`)

The CI/CD pipeline is defined in `.github/workflows/ci.yml`. It performs the following steps on every push to the `main` branch:

1.  **Checkout repository:** Gets the latest code.
2.  **Set up Python 3.11:** Configures the environment with Python 3.11.
3.  **Install dependencies:** Installs `pandas==2.3` and `ruff`.
4.  **Run Ruff linter:** Checks Python code for style and common errors, outputting results to the CI log.
5.  **Convert `data.xlsx` to `data.csv`:** Uses a Python one-liner to read the Excel file and save it as a CSV.
6.  **Execute Python script:** Runs `python execute.py` and redirects its `stdout` to `result.json`.
7.  **Setup Pages & Upload Artifact:** Prepares the GitHub Pages environment and uploads `result.json` as a deployable artifact.
8.  **Deploy to GitHub Pages:** Publishes the `result.json` artifact to GitHub Pages.

```yaml
name: CI/CD Pipeline

on:
  push:
    branches:
      - main
  workflow_dispatch: # Allows manual trigger

jobs:
  build-and-publish:
    runs-on: ubuntu-latest
    permissions:
      contents: write # Needed for checkout
      pages: write    # Needed to deploy to GitHub Pages
      id-token: write # Needed for OIDC authentication with Pages

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas==2.3 ruff

      - name: Run Ruff linter
        run: ruff check .

      - name: Convert data.xlsx to data.csv
        run: |
          python -c "
import pandas as pd
df = pd.read_excel('data.xlsx')
df.to_csv('data.csv', index=False)
print('data.xlsx converted to data.csv')
          "

      - name: Execute Python script and generate result.json
        run: python execute.py > result.json

      - name: Setup Pages
        uses: actions/configure-pages@v5

      - name: Upload result.json artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: 'result.json'

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
```

### 4. GitHub Pages

The `result.json` file is published to GitHub Pages. After a successful CI/CD run, you can access the generated JSON output at a URL similar to:

`https://<YOUR_USERNAME>.github.io/<YOUR_REPOSITORY_NAME>/result.json`

Replace `<YOUR_USERNAME>` and `<YOUR_REPOSITORY_NAME>` with your actual GitHub username and repository name.

## Local Development

To run the script and check Ruff locally:

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/<YOUR_USERNAME>/<YOUR_REPOSITORY_NAME>.git
    cd <YOUR_REPOSITORY_NAME>
    ```

2.  **Set up Python environment:**
    ```bash
    python -m venv .venv
    source .venv/bin/activate # On Windows: .venv\Scripts\activate
    pip install pandas==2.3 ruff
    ```

3.  **Convert `data.xlsx` to `data.csv`:**
    ```bash
    python -c "import pandas as pd; df = pd.read_excel('data.xlsx'); df.to_csv('data.csv', index=False)"
    ```

4.  **Run Ruff linter:**
    ```bash
    ruff check .
    ```

5.  **Execute the Python script:**
    ```bash
    python execute.py > result.json
    ```
    The `result.json` file will be created in your project root.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
